{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "## Practice Linear Regression and Hyperparameter Search\n",
    "\n",
    "\n",
    "This assignment is aimed to help you get more experience with [linear models](https://scikit-learn.org/stable/modules/linear_model.html) (especially linear regression) and [hyperparameter search](https://scikit-learn.org/stable/model_selection.html) in scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRCKrupOesLn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cz1LUfwtesLs"
   },
   "source": [
    "# Today's data\n",
    "\n",
    "400 fotos of human faces. Each face is a 2d array [64x64] of pixel brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "melNrhQoesLt"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "data = fetch_olivetti_faces().images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA2Q-6P2esLy"
   },
   "outputs": [],
   "source": [
    "# this code showcases matplotlib subplots.\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].imshow(data[i],cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6hbCClhYesL5"
   },
   "source": [
    "# Face reconstruction problem\n",
    "\n",
    "Let's solve the face reconstruction problem: given left halves of facex __(X)__, our algorithm shall predict the right half __(y)__. The idea of this approach is that left face half actually contains quite enough information to reconstruct the right face half (at least partially). Moreover in this task we'll also see, that scikit-learn linear models are capable of predicting multiple targets for a single object example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV3HsLhQesL6"
   },
   "source": [
    "Our first step is to slice the photos into X and y using slices.\n",
    "__Slices in numpy:__\n",
    "* In regular python, slice looks roughly like this: `a[2:5]` _(select elements from 2 to 5)_\n",
    "* Numpy allows you to slice N-dimensional arrays along each dimension: [image_index, height, width]\n",
    "  * `data[:10]` - Select first 10 images\n",
    "  * `data[:, :10]` - For all images, select a horizontal stripe 10 pixels high at the top of the image\n",
    "  * `data[10:20, :, -25:-15]` - Take images [10, 11, ..., 19], for each image select a _vetrical stripe_ of width 10 pixels, 15 pixels away from the _right_ side.\n",
    "\n",
    "__Your task:__\n",
    "\n",
    "Let's use slices to select all __left image halves as X__ and all __right halves as y__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUUu5T-uesL-"
   },
   "outputs": [],
   "source": [
    "# select left half of each face as X, right half as Y\n",
    "X = <Slice left half-images>\n",
    "y = <Slice right half-images>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cW4F11b_esMD"
   },
   "outputs": [],
   "source": [
    "# If you did everything right, you're gonna see left half-image and right half-image drawn separately in natural order\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[0],cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(y[0],cmap='gray')\n",
    "\n",
    "assert X.shape == y.shape == (len(data), 64, 32), \"Please slice exactly the left half-face to X and right half-face to Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2Rxit8SesMH"
   },
   "outputs": [],
   "source": [
    "def glue(left_half,right_half):\n",
    "    # merge photos back together\n",
    "    left_half = left_half.reshape([-1, 64, 32])\n",
    "    right_half = right_half.reshape([-1, 64, 32])\n",
    "    return np.concatenate([left_half, right_half],axis=-1)\n",
    "\n",
    "\n",
    "# if you did everything right, you're gonna see a valid face\n",
    "plt.imshow(glue(X, y)[99], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcvCMAogesML"
   },
   "source": [
    "# Machine learning stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_ajG2zoesMM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X.reshape([len(X), -1]),\n",
    "                                                    y.reshape([len(y), -1]),\n",
    "                                                    test_size=0.05, random_state=42)\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3timNAgJesMR"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPXoDAiqesMW"
   },
   "source": [
    "measure [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error):\n",
    "\n",
    "$$MSE(\\widehat{\\theta}) = \\mathbf{E}_{\\theta}[(\\theta - \\widehat{\\theta})^2] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PH2HuGi4esMY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_train = mean_squared_error(Y_train, model.predict(X_train))\n",
    "mae_test = mean_squared_error(Y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"Train MSE: {mse_train:.3f}\")\n",
    "print(f\"Test MSE: {mse_train:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-e00MeoesMb"
   },
   "source": [
    "## Why train error is much smaller than test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kj0_7ZZFgAya"
   },
   "outputs": [],
   "source": [
    "# Train predictions\n",
    "pics = <YOUR CODE> # reconstruct and glue together X and Y for the train dataset\n",
    "plt.figure(figsize=[16, 12])\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(pics[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCiLbOTugSV8"
   },
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "pics = <YOUR CODE> # reconstruct and glue together X and Y for the test dataset\n",
    "plt.figure(figsize=[16, 12])\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(pics[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZZF6Ns0gsIm"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPtPdBCMesMm"
   },
   "source": [
    "Remember regularisation? That is exactly what we need. There are many many linear models in sklearn package, and all of them can be found [here](https://scikit-learn.org/stable/modules/linear_model.html). We will focus on 3 of them: Ridge regression, Lasso and ElasticNet.\n",
    "Idea of all of them is very simple: Add some penalty to the objective loss function to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucmL5LNSesMm"
   },
   "source": [
    "# Ridge regression\n",
    "[RidgeRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) is just a LinearRegression, with l2 regularization - penalized for $ \\alpha \\cdot \\sum _i w_i^2$\n",
    "\n",
    "Let's train such a model with alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IxtafdhesMo"
   },
   "outputs": [],
   "source": [
    "from <YOUR CODE> import <YOUR CODE>\n",
    "\n",
    "ridge = <YOUR CODE>\n",
    "\n",
    "<YOUR CODE: fit the model on training set>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0UAnpxQesMr"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE: predict and measure MSE on train and test>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgOK0gfFiirK"
   },
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "pics = <YOUR CODE> # reconstruct and glue together X and Y for the test dataset\n",
    "plt.figure(figsize=[16, 12])\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(pics[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0xpbcZVesMy"
   },
   "source": [
    "# Grid search\n",
    "\n",
    "Train model with diferent $\\alpha$ and find one that has minimal test MSE. It's okay to use loops or any other python stuff here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qgy_LgwNesMz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8ETgdXIesM1"
   },
   "outputs": [],
   "source": [
    "def train_and_plot(model, parameter_dict):\n",
    "    \"\"\"This function takes a model and parameters\n",
    "    dict as input and plot a graph of MSE loss VS parameter value\"\"\"\n",
    "    # use GridSearchCV as before to do grid search\n",
    "    gscv = GridSearchCV(<Your code>)\n",
    "    <Fit your model>\n",
    "    plt.errorbar(gscv.param_grid['alpha'],\n",
    "                 gscv.cv_results_['mean_test_score'],\n",
    "                 gscv.cv_results_['std_test_score'],\n",
    "                 capsize=5, label=model.__str__().split(\"(\")[0])\n",
    "    plt.xscale(\"log\", nonposx='clip')\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"negative MSE\")\n",
    "    plt.grid()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AjQb9Ky2esM4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "models = <YOUR CODE> # Start from Ridge regression, but feel free to add \n",
    "                     # Lasso and ElasticNet. Note that the latter two cannot\n",
    "                     # be solved analytically and typically are much slower\n",
    "                     # to fit than Ridge regression (so you may want to limit\n",
    "                     # the number of grid points).\n",
    "\n",
    "parameters_dicts = <YOUR CODE> # It should be a list of dicts:\n",
    "                               # one parameters dict for each model\n",
    "for model, parameters_dict in zip(models, parameters_dicst):\n",
    "    train_and_plot(model, parameters_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4_oZE-CesM9"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQGIIuhbesNA"
   },
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "pics = glue(X_test, <predict with your best model>)\n",
    "plt.figure(figsize=[16, 12])\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(pics[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "433Mv13AesNG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "# Use the code you have just done to do GridSearch for Lasso and/or ElasticNet\n",
    "# models (if you haven't already). Note that Lasso and ElasticNet are much\n",
    "# slower to fit, compared to Ridge.\n",
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJkJpSK4qNzl"
   },
   "source": [
    "## Bonus part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZBAqmrrqQch"
   },
   "source": [
    "Try using `sklearn.linear_model.SGDRegressor` with `huber` loss in the code above instead of `LinearRegression`. Is it better in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xa1S9pwWesNT"
   },
   "outputs": [],
   "source": [
    "<Your code for bonus part>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. This assignment is inspired by [YSDA materials](https://github.com/yandexdataschool)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "lab2_regression_seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
